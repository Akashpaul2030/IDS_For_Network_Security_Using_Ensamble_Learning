{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "9rq0htMzOnGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7kpKtW4hsQr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6IlA3YIhCns"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypJ_kbiahE0H"
      },
      "outputs": [],
      "source": [
        "df_0 = pd.read_csv(\"/content/drive/MyDrive/IDS_Train_Dataset/KDDTrain+.txt\")\n",
        "df= df_0.copy()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7ABocmGjNWJ"
      },
      "outputs": [],
      "source": [
        "columns = (['duration'\n",
        ",'protocol_type'\n",
        ",'service'\n",
        ",'flag'\n",
        ",'src_bytes'\n",
        ",'dst_bytes'\n",
        ",'land'\n",
        ",'wrong_fragment'\n",
        ",'urgent'\n",
        ",'hot'\n",
        ",'num_failed_logins'\n",
        ",'logged_in'\n",
        ",'num_compromised'\n",
        ",'root_shell'\n",
        ",'su_attempted'\n",
        ",'num_root'\n",
        ",'num_file_creations'\n",
        ",'num_shells'\n",
        ",'num_access_files'\n",
        ",'num_outbound_cmds'\n",
        ",'is_host_login'\n",
        ",'is_guest_login'\n",
        ",'count'\n",
        ",'srv_count'\n",
        ",'serror_rate'\n",
        ",'srv_serror_rate'\n",
        ",'rerror_rate'\n",
        ",'srv_rerror_rate'\n",
        ",'same_srv_rate'\n",
        ",'diff_srv_rate'\n",
        ",'srv_diff_host_rate'\n",
        ",'dst_host_count'\n",
        ",'dst_host_srv_count'\n",
        ",'dst_host_same_srv_rate'\n",
        ",'dst_host_diff_srv_rate'\n",
        ",'dst_host_same_src_port_rate'\n",
        ",'dst_host_srv_diff_host_rate'\n",
        ",'dst_host_serror_rate'\n",
        ",'dst_host_srv_serror_rate'\n",
        ",'dst_host_rerror_rate'\n",
        ",'dst_host_srv_rerror_rate'\n",
        ",'attack'\n",
        ",'level'])\n",
        "\n",
        "df.columns = columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-wI56CBjNTy"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcH19IOtjkLE"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ars9HqjdjkIV"
      },
      "outputs": [],
      "source": [
        "#helper function for deeper analysis\n",
        "def unique_values(df, columns):\n",
        "    \"\"\"Prints unique values and their counts for specific columns in the DataFrame.\"\"\"\n",
        "\n",
        "    for column_name in columns:\n",
        "        print(f\"Column: {column_name}\\n{'-'*30}\")\n",
        "        unique_vals = df[column_name].unique()\n",
        "        value_counts = df[column_name].value_counts()\n",
        "        print(f\"Unique Values ({len(unique_vals)}): {unique_vals}\\n\")\n",
        "        print(f\"Value Counts:\\n{value_counts}\\n{'='*40}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOOkh2OFj53G"
      },
      "outputs": [],
      "source": [
        "cat_features = df.select_dtypes(include='object').columns\n",
        "unique_values(df, cat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K49ViI5tj50P"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OAcPbglkJhC"
      },
      "outputs": [],
      "source": [
        "# Check if the column names are correctly assigned\n",
        "print(df.columns)\n",
        "\n",
        "# Create the new 'attack' column with values 'normal' or 'attack'\n",
        "attack_n = []\n",
        "for i in df['attack']:\n",
        "    if i == 'normal':\n",
        "        attack_n.append(\"normal\")\n",
        "    else:\n",
        "        attack_n.append(\"attack\")\n",
        "df['attack'] = attack_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfuyJCqJJZ5u"
      },
      "outputs": [],
      "source": [
        "# Check column names in the DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# If the column name is correct\n",
        "attack_n = []\n",
        "for i in df['attack']:\n",
        "    if i == 'normal':\n",
        "        attack_n.append(\"normal\")\n",
        "    else:\n",
        "        attack_n.append(\"attack\")\n",
        "df['attack'] = attack_n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeU-CJNLkUer"
      },
      "outputs": [],
      "source": [
        "df['attack'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0QDFNskkYdR"
      },
      "outputs": [],
      "source": [
        "df.hist(bins=43,figsize=(20,30));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu2A2mHYkobk"
      },
      "outputs": [],
      "source": [
        "# Define a custom color palette\n",
        "palette = {'tcp': 'skyblue', 'udp': 'orange', 'icmp': 'green'}\n",
        "\n",
        "plt.figure(figsize=(16,4))\n",
        "sns.countplot(x='attack', data=df, hue='protocol_type', palette=palette)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Attack Counts over Protocol Types', fontdict={'fontsize': 16})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhjQVaE9lqbO"
      },
      "outputs": [],
      "source": [
        "# So we can see that most of the attacks are from tcp, then udp, and least attack comes from icmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g-XhGWilrBQ"
      },
      "outputs": [],
      "source": [
        "df[\"protocol_type\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-lAucvjlxXG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='is_guest_login', hue='attack', data=df, palette='Set2')\n",
        "plt.xlabel('Is Guest Login')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Attack Types by Guest Login')\n",
        "plt.legend(title='Attack Type')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y42rNTyko2QR"
      },
      "outputs": [],
      "source": [
        "cat_features = df.select_dtypes(include='object').columns\n",
        "cat_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cciuOZpo78b"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "le=preprocessing.LabelEncoder()\n",
        "clm=['protocol_type', 'service', 'flag', 'attack']\n",
        "for x in clm:\n",
        "    df[x]=le.fit_transform(df[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65uw74dcpBgf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop([\"attack\"], axis=1)\n",
        "y = df[\"attack\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.1,random_state=43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwValFHOpD3G"
      },
      "outputs": [],
      "source": [
        "train_index = X_train.columns\n",
        "train_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5AY1Aydrmr5"
      },
      "source": [
        "**Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwMaZcM4LBId"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train and y_train are already defined and preprocessed\n",
        "# Example:\n",
        "# X_train = df.drop('attack', axis=1)\n",
        "# y_train = df['attack']\n",
        "\n",
        "# Apply Chi-Square feature selection\n",
        "chi_scores, p_values = chi2(X_train, y_train)\n",
        "\n",
        "# Create a Pandas Series with the Chi-Square scores\n",
        "chi_scores = pd.Series(chi_scores)\n",
        "chi_scores.index = X_train.columns  # Ensure X_train is a DataFrame with column names\n",
        "\n",
        "# Sort the Chi-Square scores in descending order\n",
        "chi_scores_sorted = chi_scores.sort_values(ascending=False)\n",
        "\n",
        "# Display the sorted Chi-Square scores\n",
        "print(chi_scores_sorted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7AMAAZ_OKpl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Calculate mutual information scores\n",
        "mutual_info = mutual_info_classif(X_train, y_train)\n",
        "mutual_info_series = pd.Series(mutual_info, index=X_train.columns)\n",
        "mutual_info_series.sort_values(ascending=False).plot.bar(figsize=(20, 5))\n",
        "plt.title('Feature Importance Scores based on Mutual Information')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yB7Y5NiOVWt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scale features to non-negative values\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Calculate Chi-square scores\n",
        "chi_scores, p_values = chi2(X_train_scaled, y_train)\n",
        "chi_scores_series = pd.Series(chi_scores, index=X_train.columns)\n",
        "chi_scores_series.sort_values(ascending=False).plot.bar(figsize=(20, 5))\n",
        "plt.title('Feature Importance Scores based on Chi-Square Test')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi0mjN--OoJy"
      },
      "outputs": [],
      "source": [
        "# Calculate Mutual Information scores\n",
        "mutual_info_scores = mutual_info_classif(X_train, y_train)\n",
        "mutual_info_series = pd.Series(mutual_info_scores, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "# Calculate Chi-Square scores\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "chi_scores, p_values = chi2(X_train_scaled, y_train)\n",
        "chi_scores_series = pd.Series(chi_scores, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "# Plot both for comparison\n",
        "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
        "mutual_info_series.plot.bar(ax=axes[0], title='Mutual Information Feature Importance')\n",
        "chi_scores_series.plot.bar(ax=axes[1], title='Chi-Square Feature Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsWgD1eUPpFQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif, chi2, SelectKBest\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Select features using Mutual Information\n",
        "mi_selector = SelectKBest(mutual_info_classif, k=10)\n",
        "X_mi = mi_selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Select features using Chi-Square\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "chi_selector = SelectKBest(chi2, k=10)\n",
        "X_chi = chi_selector.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "# Train a model using Mutual Information features\n",
        "rf_mi = RandomForestClassifier(random_state=42)\n",
        "rf_mi.fit(X_mi, y_train)\n",
        "y_pred_mi = rf_mi.predict(mi_selector.transform(X_test))\n",
        "\n",
        "# Train a model using Chi-Square features\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "rf_chi = RandomForestClassifier(random_state=42)\n",
        "rf_chi.fit(X_chi, y_train)\n",
        "y_pred_chi = rf_chi.predict(chi_selector.transform(X_test_scaled))\n",
        "\n",
        "# Evaluate the models\n",
        "print(\"Mutual Information Features Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_mi))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mi))\n",
        "\n",
        "print(\"\\nChi-Square Features Model Performance:\")\n",
        "print(classification_report(y_test, y_pred_chi))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_chi))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSEAsqzvq-O3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "mutual_info = mutual_info_classif(X_train, y_train)\n",
        "mutual_info = pd.Series(mutual_info)\n",
        "mutual_info.index = train_index\n",
        "mutual_info.sort_values(ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuVwwxRqrv2N"
      },
      "outputs": [],
      "source": [
        "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 5));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eQp6r8jEF7N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j46cmDqrr9Tw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "Select_features = SelectKBest(mutual_info_classif, k=30)\n",
        "Select_features.fit(X_train, y_train)\n",
        "train_index[Select_features.get_support()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2NU1TRKsKKp"
      },
      "outputs": [],
      "source": [
        "columns=['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
        "       'dst_bytes', 'wrong_fragment', 'hot', 'logged_in', 'num_compromised',\n",
        "       'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate']\n",
        "\n",
        "#We will continue our model with top 15 features, because dataset is big enough\n",
        "\n",
        "X_train=X_train[columns]\n",
        "X_test=X_test[columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhs6Hhe4sOPB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test) # we use only transform in order to prevent data leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiusGKhSENiB"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy0xHd8XsOMf"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoWOY_IJsWZ-"
      },
      "outputs": [],
      "source": [
        "XGBoost_model = XGBClassifier(random_state = 42)\n",
        "Logistic_model = LogisticRegression(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGljyvAFsXrG"
      },
      "outputs": [],
      "source": [
        "XGBoost = XGBoost_model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC414b6tsXkD"
      },
      "outputs": [],
      "source": [
        "Logistic = Logistic_model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4vGvJbvsd5K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVV4ReortYsM"
      },
      "outputs": [],
      "source": [
        "#it's a helper function in order to evaluate our model if it's overfit or underfit.\n",
        "def eval_metric(model, X_train, y_train, X_test, y_test):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"Test_Set\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print()\n",
        "    print(\"Train_Set\")\n",
        "    print(confusion_matrix(y_train, y_train_pred))\n",
        "    print(classification_report(y_train, y_train_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf1EUoIztavr"
      },
      "outputs": [],
      "source": [
        "eval_metric(Logistic_model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esPXUIUVtg5Z"
      },
      "outputs": [],
      "source": [
        "eval_metric(XGBoost_model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsP5sayVtit9"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Train the models\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"Test Set Evaluation\")\n",
        "    print(confusion_matrix(y_test, y_test_pred))\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    print(\"Train Set Evaluation\")\n",
        "    print(confusion_matrix(y_train, y_train_pred))\n",
        "    print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print(\"Random Forest Evaluation:\")\n",
        "evaluate_model(rf_model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Evaluate XGBoost\n",
        "print(\"\\nXGBoost Evaluation:\")\n",
        "evaluate_model(xgb_model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPp7_mdRwcrb"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"n_estimators\": [50,64,100,128],\n",
        "    \"max_depth\": [2, 3, 4,5,6],\n",
        "    \"learning_rate\": [0.01,0,0.03, 0.05, 0.1],\n",
        "    \"subsample\": [0.5, 0.8],\n",
        "    \"colsample_bytree\": [0.5, 0.8]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assume the dataset (X, y) has already been preprocessed\n",
        "# Example dataset split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional depending on model requirements)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model definitions\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Ensemble Voting Classifier (Soft Voting)\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', rf_clf),\n",
        "    ('xgb', xgb_clf)],\n",
        "    voting='soft')\n",
        "\n",
        "# Train the model\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix Generation\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize Confusion Matrix using heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Attack'], yticklabels=['Normal', 'Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Voting Classifier')\n",
        "plt.show()\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'Attack']))\n",
        "\n",
        "# Optionally, save the confusion matrix figure\n",
        "plt.savefig(\"confusion_matrix_voting_classifier.png\")\n"
      ],
      "metadata": {
        "id": "1s7pxgZWJ-Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ANN Model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_ann = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_ann))\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_ann))\n",
        "\n",
        "# Accuracy\n",
        "ann_accuracy = accuracy_score(y_test, y_pred_ann)\n",
        "print(f\"ANN Accuracy: {ann_accuracy:.4f}\")\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r_mvRT8bOxQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kudDD0xKwgGr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "XGB_model = XGBClassifier(random_state=42) #initialize the model\n",
        "\n",
        "XGB_grid_model = GridSearchCV(XGB_model,\n",
        "                        param_grid,\n",
        "                        scoring=\"f1\",\n",
        "                        n_jobs=-1,\n",
        "                        return_train_score=True).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY0eYYGTIQgK"
      },
      "outputs": [],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo031OMDINW0"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to balance the training dataset\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the new size of the training data after applying SMOTE\n",
        "print(\"Training set size after SMOTE (X_train_res):\", X_train_res.shape)\n",
        "print(\"Training target size after SMOTE (y_train_res):\", y_train_res.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmc_FiIHIv0I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize the class distribution before and after SMOTE\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Before SMOTE\n",
        "sns.countplot(x=y_train, ax=ax[0], palette=\"Set2\")\n",
        "ax[0].set_title(\"Class Distribution Before SMOTE\")\n",
        "ax[0].set_xlabel(\"Classes\")\n",
        "ax[0].set_ylabel(\"Count\")\n",
        "\n",
        "# After SMOTE\n",
        "sns.countplot(x=y_train_res, ax=ax[1], palette=\"Set2\")\n",
        "ax[1].set_title(\"Class Distribution After SMOTE\")\n",
        "ax[1].set_xlabel(\"Classes\")\n",
        "ax[1].set_ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-_Z31RMwesB"
      },
      "outputs": [],
      "source": [
        "XGB_grid_model.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB3P_yvx4f7-"
      },
      "outputs": [],
      "source": [
        "XGB_grid_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f8mXVgi4oEq"
      },
      "outputs": [],
      "source": [
        "#final model\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "XGB_model = XGBClassifier(\n",
        "    colsample_bytree=0.5,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    n_estimators=128,\n",
        "    subsample=0.8\n",
        ")\n",
        "\n",
        "# Fit the classifier to your data\n",
        "XGB_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THNYJ5T98jk1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, roc_auc_score\n",
        "\n",
        "# Make predictions with the Random Forest model\n",
        "y_pred_rf = XGB_model.predict(X_test)\n",
        "y_pred_proba_rf = XGB_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate evaluation metrics for the Random Forest model\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_auc = roc_auc_score(y_test, y_pred_proba_rf[:, 1])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
        "print(f\"Random Forest Recall: {rf_recall}\")\n",
        "print(f\"Random Forest AUC: {rf_auc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFtJDL8W9Rd8"
      },
      "outputs": [],
      "source": [
        "eval_metric(XGB_model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0hEL2af9kIY"
      },
      "outputs": [],
      "source": [
        "model = XGB_model\n",
        "model.feature_importances_\n",
        "\n",
        "feats = pd.DataFrame(index=X[columns].columns, data= model.feature_importances_, columns=['XGB_importance'])\n",
        "ada_imp_feats = feats.sort_values(\"XGB_importance\", ascending = False)\n",
        "ada_imp_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAyfPh3jGA7w"
      },
      "outputs": [],
      "source": [
        "## Ensamble learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob3228peGEqq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Define individual models\n",
        "logistic_model = LogisticRegression(random_state=42)\n",
        "xgboost_model = XGBClassifier(random_state=42)\n",
        "svm_model = SVC(probability=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOvllrSsGG4R"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression + XGBoost Ensemble\n",
        "ensemble_log_xgb = VotingClassifier(estimators=[\n",
        "    ('logistic', logistic_model),\n",
        "    ('xgboost', xgboost_model)\n",
        "], voting='soft')\n",
        "\n",
        "# SVM + XGBoost Ensemble\n",
        "ensemble_svm_xgb = VotingClassifier(estimators=[\n",
        "    ('svm', svm_model),\n",
        "    ('xgboost', xgboost_model)\n",
        "], voting='soft')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOCpgnR9GJMJ"
      },
      "outputs": [],
      "source": [
        "# Train ensembles\n",
        "ensemble_log_xgb.fit(X_train, y_train)\n",
        "ensemble_svm_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_log_xgb = ensemble_log_xgb.predict(X_test)\n",
        "y_pred_svm_xgb = ensemble_svm_xgb.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Logistic Regression + XGBoost Ensemble\")\n",
        "print(confusion_matrix(y_test, y_pred_log_xgb))\n",
        "print(classification_report(y_test, y_pred_log_xgb))\n",
        "\n",
        "print(\"SVM + XGBoost Ensemble\")\n",
        "print(confusion_matrix(y_test, y_pred_svm_xgb))\n",
        "print(classification_report(y_test, y_pred_svm_xgb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnvmUA_FMgex"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define individual models\n",
        "logistic_model = LogisticRegression(random_state=42)\n",
        "xgboost_model = XGBClassifier(random_state=42)\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Logistic Regression + XGBoost Ensemble\n",
        "ensemble_log_xgb = VotingClassifier(estimators=[\n",
        "    ('logistic', logistic_model),\n",
        "    ('xgboost', xgboost_model)\n",
        "], voting='soft')\n",
        "\n",
        "# SVM + XGBoost Ensemble\n",
        "ensemble_svm_xgb = VotingClassifier(estimators=[\n",
        "    ('svm', svm_model),\n",
        "    ('xgboost', xgboost_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Random Forest + XGBoost Ensemble\n",
        "ensemble_rf_xgb = VotingClassifier(estimators=[\n",
        "    ('random_forest', random_forest_model),\n",
        "    ('xgboost', xgboost_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Train ensembles\n",
        "ensemble_log_xgb.fit(X_train, y_train)\n",
        "ensemble_svm_xgb.fit(X_train, y_train)\n",
        "ensemble_rf_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_log_xgb = ensemble_log_xgb.predict(X_test)\n",
        "y_pred_svm_xgb = ensemble_svm_xgb.predict(X_test)\n",
        "y_pred_rf_xgb = ensemble_rf_xgb.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model_name, y_test, y_pred):\n",
        "    print(f\"{model_name} Ensemble\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "evaluate_model(\"Logistic Regression + XGBoost\", y_test, y_pred_log_xgb)\n",
        "evaluate_model(\"SVM + XGBoost\", y_test, y_pred_svm_xgb)\n",
        "evaluate_model(\"Random Forest + XGBoost\", y_test, y_pred_rf_xgb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHUwnOnLR7_o"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define individual models\n",
        "logistic_model = LogisticRegression(random_state=42)\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "naive_bayes_model = GaussianNB()\n",
        "\n",
        "# Create ensemble model\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('logistic', logistic_model),\n",
        "    ('svm', svm_model),\n",
        "    ('decision_tree', decision_tree_model),\n",
        "    ('naive_bayes', naive_bayes_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Train ensemble model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "print(\"Ensemble Model\")\n",
        "print(confusion_matrix(y_test, y_pred_ensemble))\n",
        "print(classification_report(y_test, y_pred_ensemble))\n",
        "\n",
        "# ROC Curve for the ensemble model\n",
        "y_pred_prob_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
        "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, y_pred_prob_ensemble)\n",
        "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_ensemble, tpr_ensemble, color='blue', lw=2, label='Ensemble (AUC = %0.2f)' % roc_auc_ensemble)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Ensemble Model')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall Curve for the ensemble model\n",
        "precision_ensemble, recall_ensemble, _ = precision_recall_curve(y_test, y_pred_prob_ensemble)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall_ensemble, precision_ensemble, color='blue', lw=2, label='Ensemble')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Ensemble Model')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue1kSJiHw9fq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have trained a model named 'ensemble_model'\n",
        "# and have X_train, X_test, y_train, y_test as your data splits\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = ensemble_model.predict(X_train)\n",
        "y_test_pred = ensemble_model.predict(X_test)\n",
        "\n",
        "# Generate confusion matrices\n",
        "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# Plot confusion matrix for training data\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - Training Data')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Plot confusion matrix for testing data\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - Testing Data')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classification reports\n",
        "print(\"Classification Report - Training Data\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "print(\"Classification Report - Testing Data\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# ROC Curve for the ensemble model\n",
        "y_pred_prob_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
        "fpr_ensemble, tpr_ensemble, _ = roc_curve(y_test, y_pred_prob_ensemble)\n",
        "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_ensemble, tpr_ensemble, color='blue', lw=2, label='Ensemble (AUC = %0.2f)' % roc_auc_ensemble)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Ensemble Model')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall Curve for the ensemble model\n",
        "precision_ensemble, recall_ensemble, _ = precision_recall_curve(y_test, y_pred_prob_ensemble)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall_ensemble, precision_ensemble, color='blue', lw=2, label='Ensemble')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Ensemble Model')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "models = ['SVM', 'SVM +\\nXGBoost', 'Decision Tree +\\nXGBoost', 'XGBoost', 'Random Forest +\\nXGBoost']\n",
        "metric1 = [0.9844, 0.9965, 0.9946, 0.9834, 0.9986]\n",
        "metric2 = [0.9835, 0.9953, 0.9929, 0.9848, 0.9986]\n",
        "metric3 = [0.9876, 0.9982, 0.9970, 0.9868, 0.9986]\n",
        "\n",
        "x = np.arange(len(models))  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "# Create figure and axis objects\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot bars\n",
        "rects1 = ax.bar(x - width, metric1, width, label='Metric 1', color='#4e79a7')\n",
        "rects2 = ax.bar(x, metric2, width, label='Metric 2', color='#f28e2b')\n",
        "rects3 = ax.bar(x + width, metric3, width, label='Metric 3', color='#e15759')\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_ylabel('Performance Score')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "# Set y-axis limits\n",
        "ax.set_ylim(0.98, 1.001)\n",
        "\n",
        "# Add value labels on the bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.4f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', rotation=90, fontsize=8)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Uncomment the following line to save the figure\n",
        "# plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "ZLS9C6zr8pOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Assuming you have your data loaded as X and y\n",
        "# If X is a DataFrame, convert it to a numpy array\n",
        "if isinstance(X, pd.DataFrame):\n",
        "    feature_names = X.columns\n",
        "    X = X.values\n",
        "else:\n",
        "    feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection\n",
        "select_features = SelectKBest(mutual_info_classif, k=30)\n",
        "X_train_selected = select_features.fit_transform(X_train, y_train)\n",
        "X_test_selected = select_features.transform(X_test)\n",
        "\n",
        "# Get selected feature indices\n",
        "selected_indices = select_features.get_support(indices=True)\n",
        "\n",
        "# Get selected feature names (limited to top 15)\n",
        "selected_features = [feature_names[i] for i in selected_indices][:15]\n",
        "\n",
        "# Limit to top 15 features\n",
        "X_train_selected = X_train_selected[:, :15]\n",
        "X_test_selected = X_test_selected[:, :15]\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# Define models (added Decision Tree)\n",
        "models = {\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)  # Added Decision Tree\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nSelected features:\")\n",
        "print(selected_features)\n"
      ],
      "metadata": {
        "id": "KsFf3cj7-m6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Assuming X and y are already defined\n",
        "# If X is a DataFrame, convert it to a numpy array\n",
        "if isinstance(X, pd.DataFrame):\n",
        "    feature_names = X.columns\n",
        "    X = X.values\n",
        "else:\n",
        "    feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Temporarily using the first 15 features for simplicity\n",
        "X_train = X_train[:, :15]\n",
        "X_test = X_test[:, :15]\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Train models and calculate metrics\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'Accuracy': accuracy\n",
        "    })\n",
        "\n",
        "# Add the additional models' results\n",
        "additional_results = [\n",
        "    {'Model': 'SVM', 'Precision': 0.9844, 'Recall': 0.9835, 'F1 Score': 0.9876, 'Accuracy': None},\n",
        "    {'Model': 'SVM + XGBoost', 'Precision': 0.9965, 'Recall': 0.9953, 'F1 Score': 0.9982, 'Accuracy': None},\n",
        "    {'Model': 'Decision Tree + XGBoost', 'Precision': 0.9946, 'Recall': 0.9929, 'F1 Score': 0.9970, 'Accuracy': None},\n",
        "    {'Model': 'XGBoost', 'Precision': 0.9834, 'Recall': 0.9848, 'F1 Score': 0.9868, 'Accuracy': None},\n",
        "    {'Model': 'Random Forest + XGBoost', 'Precision': 0.9986, 'Recall': 0.9986, 'F1 Score': 0.9986, 'Accuracy': None}\n",
        "]\n",
        "\n",
        "# Append the additional results to the results list\n",
        "results.extend(additional_results)\n",
        "\n",
        "# Create DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Create visualization for Precision, Recall, and F1-Score\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Melt the dataframe for easier plotting\n",
        "melted_df = pd.melt(results_df, id_vars=['Model'], value_vars=['Precision', 'Recall', 'F1 Score'], var_name='Metric', value_name='Score')\n",
        "\n",
        "# Create the grouped bar plot\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=melted_df)\n",
        "\n",
        "# Zoom in on the y-axis (for example, focusing on 0.85 to 1.0)\n",
        "plt.ylim([0.85, 1.0])\n",
        "\n",
        "plt.title('Zoomed Model Comparison: Precision, Recall, and F1 Score', fontsize=16)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Metric', title_fontsize=12, fontsize=10)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the Accuracy for each model (ignore for additional models where Accuracy is None)\n",
        "print(\"\\nAccuracy for each model:\")\n",
        "for index, row in results_df.iterrows():\n",
        "    if row['Accuracy'] is not None:\n",
        "        print(f\"{row['Model']}: {row['Accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "hEKjHZHrFZuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}